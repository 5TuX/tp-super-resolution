{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92ec3f2-c118-457e-8b10-ccbbcf1eb7ef",
   "metadata": {},
   "source": [
    "# Deep Learning-Based Image Super-Resolution in LAB Color Space\n",
    "\n",
    "This notebook demonstrates how to use deep learning to improve the resolution of images. We'll use the LAB color space, which separates lightness from color, making it easier for our model to focus on important details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training\n",
    "config = {\n",
    "    \"batch_size\": 16,\n",
    "    \"adam_lr\": 1e-3,\n",
    "    \"loss_type\": \"mse\",  # or 'lpips'\n",
    "    \"resume_weights_path\": None,  # Path to checkpoint to resume from\n",
    "    \"num_epochs\": 10,\n",
    "    \"train_batches_to_load\": 64,\n",
    "    \"validation_batches_to_load\": 16,\n",
    "    \"overfit_one_batch\": False,\n",
    "    \"max_samples\": None,  # Set to an integer to limit training samples\n",
    "    \"visualize_every_n_epochs\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9497c2",
   "metadata": {},
   "source": [
    "## What is Image Super-Resolution?\n",
    "Image super-resolution is the process of taking a blurry or low-resolution image and making it clearer and sharper. This is useful in many fields, like medical imaging, security, and photography."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f2047",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Let's start by importing the libraries we need. These include tools for deep learning (PyTorch), image processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55834149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "from skimage import color\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import lpips\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d1506",
   "metadata": {},
   "source": [
    "## Why Use the LAB Color Space?\n",
    "The LAB color space separates the lightness (L) from the color information (A and B channels). This helps our model focus on making the image brighter and clearer, while keeping the colors accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb85fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lab_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    arr = np.asarray(img.convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "    lab = color.rgb2lab(arr)\n",
    "    l_channel = lab[..., 0:1] / 100.0\n",
    "    a = lab[..., 1:2] / 128.0\n",
    "    b = lab[..., 2:3] / 128.0\n",
    "    lab_norm = np.concatenate([l_channel, a, b], axis=-1)\n",
    "    return torch.from_numpy(lab_norm.transpose(2, 0, 1).copy()).float()\n",
    "\n",
    "\n",
    "def to_numpy_img(lab_tensor: torch.Tensor) -> np.ndarray:\n",
    "    arr = lab_tensor.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    l_channel = arr[..., 0] * 100.0\n",
    "    a = arr[..., 1] * 128.0\n",
    "    b = arr[..., 2] * 128.0\n",
    "    lab = np.stack([l_channel, a, b], axis=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        rgb = color.lab2rgb(lab)\n",
    "    return np.clip(rgb, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdf12b",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "We will use a large image dataset and process it so that the model can learn to turn low-resolution images into high-resolution ones. We'll crop, resize, and convert images to LAB format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stream(dataset, crop_size: int, scale: int):\n",
    "    lowres_size = crop_size // scale\n",
    "    crop = transforms.RandomCrop(crop_size)\n",
    "    for example in dataset:\n",
    "        img_data = example[\"image\"]\n",
    "        img = (\n",
    "            img_data\n",
    "            if isinstance(img_data, Image.Image)\n",
    "            else Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
    "        )\n",
    "        if min(img.size) < crop_size:\n",
    "            scale_factor = crop_size / min(img.size)\n",
    "            new_size = (\n",
    "                int(round(img.size[0] * scale_factor)),\n",
    "                int(round(img.size[1] * scale_factor)),\n",
    "            )\n",
    "            img = img.resize(new_size, resample=Image.Resampling.BICUBIC)\n",
    "        img_patch = crop(img)\n",
    "        lab_patch = to_lab_tensor(img_patch)\n",
    "        lab_patch_lowres = torch.nn.functional.interpolate(\n",
    "            lab_patch.unsqueeze(0),\n",
    "            size=(lowres_size, lowres_size),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze(0)\n",
    "        yield lab_patch_lowres, lab_patch\n",
    "\n",
    "\n",
    "class SuperResStream(IterableDataset):\n",
    "    def __init__(self, dataset, crop_size: int, scale: int):\n",
    "        self.dataset = dataset\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "\n",
    "    def __iter__(self):\n",
    "        return preprocess_stream(self.dataset, self.crop_size, self.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1a7f1",
   "metadata": {},
   "source": [
    "## Load the Datasets\n",
    "Let's load the training and validation datasets using Hugging Face Datasets. We'll use streaming mode to avoid downloading the entire dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"imagenet-1k\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "val_dataset = load_dataset(\n",
    "    \"imagenet-1k\",\n",
    "    split=\"validation\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 128\n",
    "scale = 2\n",
    "train_stream = SuperResStream(train_dataset, crop_size, scale)\n",
    "val_stream = SuperResStream(val_dataset, crop_size, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_stream,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_stream,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09d928",
   "metadata": {},
   "source": [
    "## Build the Deep Super-Resolution Model\n",
    "We'll use a deep neural network to predict the high-resolution version of the image. The model focuses on the L (lightness) channel, which is most important for sharpness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSuperResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        body_layers = []\n",
    "        for _ in range(8):\n",
    "            body_layers.append(nn.Conv2d(128, 128, kernel_size=3, padding=1))\n",
    "            body_layers.append(nn.ReLU(inplace=True))\n",
    "        self.body = nn.Sequential(*body_layers)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.tail = nn.Conv2d(\n",
    "            128,\n",
    "            1,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.head(x)\n",
    "        x = self.body(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b99706",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepSuperResNet().to(device)\n",
    "print(\"Model summary:\")\n",
    "print(\n",
    "    summary(\n",
    "        model,\n",
    "        input_size=(config[\"batch_size\"], 3, 64, 64),\n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    "        depth=4,\n",
    "        row_settings=(\"var_names\",),\n",
    "    )\n",
    ")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"adam_lr\", 1e-3))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba85f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"loss_type\"] == \"lpips\":\n",
    "    lpips_loss_fn = lpips.LPIPS(net=\"vgg\", spatial=False).to(device)\n",
    "\n",
    "    def criterion(pred, target):\n",
    "        pred = 2 * pred - 1\n",
    "        target = 2 * target - 1\n",
    "        return lpips_loss_fn(pred, target).mean()\n",
    "elif config[\"loss_type\"] == \"mse\":\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    def criterion(pred, target):\n",
    "        return mse_loss_fn(pred, target)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown loss_type: {config['loss_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c88e84",
   "metadata": {},
   "source": [
    "## Training Loop: One Epoch\n",
    "Let's run one epoch of training. We'll process a set number of batches, update the model, and keep track of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "running_loss, batch_count = 0.0, 0\n",
    "train_iterator = iter(train_loader)\n",
    "last_lowres, last_highres = None, None\n",
    "for _ in range(config[\"train_batches_to_load\"]):\n",
    "    try:\n",
    "        lowres, highres = next(train_iterator)\n",
    "    except StopIteration:\n",
    "        train_iterator = iter(train_loader)\n",
    "        lowres, highres = next(train_iterator)\n",
    "    lowres = lowres.to(device)\n",
    "    highres = highres.to(device)\n",
    "    upsampled = torch.nn.functional.interpolate(\n",
    "        lowres,\n",
    "        size=highres.shape[-2:],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "    output_L = model(upsampled)\n",
    "    highres_L = highres[:, 0:1]\n",
    "    if output_L.shape[-2:] != highres_L.shape[-2:]:\n",
    "        output_L = torch.nn.functional.interpolate(\n",
    "            output_L,\n",
    "            size=highres_L.shape[-2:],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "    loss = criterion(output_L, highres_L)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    running_loss += loss.item()\n",
    "    batch_count += 1\n",
    "    last_lowres, last_highres = (\n",
    "        lowres.detach().cpu(),\n",
    "        highres.detach().cpu(),\n",
    "    )\n",
    "    print(f\"Batch {_+1}/{config['train_batches_to_load']}, Loss: {loss.item():.4f}\")\n",
    "avg_loss = running_loss / batch_count if batch_count > 0 else float(\"nan\")\n",
    "print(f\"Average training loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26275b21",
   "metadata": {},
   "source": [
    "## Validation Loop: One Epoch\n",
    "Now let's evaluate the model on the validation set for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef89fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_running_loss, val_batch_count = 0.0, 0\n",
    "val_iterator = iter(val_loader)\n",
    "val_last_lowres, val_last_highres = None, None\n",
    "with torch.no_grad():\n",
    "    for _ in range(config[\"validation_batches_to_load\"]):\n",
    "        try:\n",
    "            val_lowres, val_highres = next(val_iterator)\n",
    "        except StopIteration:\n",
    "            val_iterator = iter(val_loader)\n",
    "            val_lowres, val_highres = next(val_iterator)\n",
    "        val_lowres = val_lowres.to(device)\n",
    "        val_highres = val_highres.to(device)\n",
    "        val_upsampled = torch.nn.functional.interpolate(\n",
    "            val_lowres,\n",
    "            size=val_highres.shape[-2:],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        val_output_L = model(val_upsampled)\n",
    "        val_highres_L = val_highres[:, 0:1]\n",
    "        if val_output_L.shape[-2:] != val_highres_L.shape[-2:]:\n",
    "            val_output_L = torch.nn.functional.interpolate(\n",
    "                val_output_L,\n",
    "                size=val_highres_L.shape[-2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        val_loss = criterion(val_output_L, val_highres_L)\n",
    "        val_running_loss += val_loss.item()\n",
    "        val_batch_count += 1\n",
    "        val_last_lowres, val_last_highres = (\n",
    "            val_lowres.detach().cpu(),\n",
    "            val_highres.detach().cpu(),\n",
    "        )\n",
    "        print(\n",
    "            f\"Val Batch {_+1}/{config['validation_batches_to_load']}, Loss: {val_loss.item():.4f}\"\n",
    "        )\n",
    "val_avg_loss = (\n",
    "    val_running_loss / val_batch_count if val_batch_count > 0 else float(\"nan\")\n",
    ")\n",
    "print(f\"Average validation loss: {val_avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d605d9a",
   "metadata": {},
   "source": [
    "## Visualize Progress\n",
    "Let's visualize the results of the last batch from both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_progress(model, lowres, highres, device, title=\"Output\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lowres_AB = lowres[:, 1:3]\n",
    "        highres_L = highres[:, 0:1]\n",
    "        upsampled = torch.nn.functional.interpolate(\n",
    "            lowres,\n",
    "            size=highres.shape[-2:],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        pred_L = model(upsampled.to(device)).cpu()\n",
    "        up_AB = torch.nn.functional.interpolate(\n",
    "            lowres_AB,\n",
    "            size=highres_L.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        if pred_L.shape[-2:] != up_AB.shape[-2:]:\n",
    "            pred_L = torch.nn.functional.interpolate(\n",
    "                pred_L,\n",
    "                size=up_AB.shape[-2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        outputs = torch.cat([pred_L, up_AB], dim=1)\n",
    "        n = min(6, lowres.size(0))\n",
    "        fig, axes = plt.subplots(3, n, figsize=(2.5 * n, 8))\n",
    "        for i in range(n):\n",
    "            axes[0, i].imshow(to_numpy_img(lowres[i]))\n",
    "            axes[0, i].set_title(\"Low-res\", fontsize=10)\n",
    "            axes[1, i].imshow(to_numpy_img(outputs[i]))\n",
    "            axes[1, i].set_title(\"Output\", fontsize=10)\n",
    "            axes[2, i].imshow(to_numpy_img(highres[i]))\n",
    "            axes[2, i].set_title(\"High-res\", fontsize=10)\n",
    "            for row in range(3):\n",
    "                axes[row, i].axis(\"off\")\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc185f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_progress(\n",
    "    model, last_lowres, last_highres, device, title=\"Training Batch Output\"\n",
    ")\n",
    "visualize_progress(\n",
    "    model, val_last_lowres, val_last_highres, device, title=\"Validation Batch Output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbf0c3",
   "metadata": {},
   "source": [
    "## Save Model Weights\n",
    "Let's save the model weights after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = Path(\"weights\")\n",
    "weights_dir.mkdir(exist_ok=True)\n",
    "weights_path = weights_dir / \"superres_model_final.pth\"\n",
    "torch.save(model.state_dict(), str(weights_path))\n",
    "print(f\"Model weights saved to {weights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadcf7cb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, you learned how to:\n",
    "- Prepare images for super-resolution using the LAB color space\n",
    "- Build a deep neural network to improve image resolution\n",
    "- Visualize and evaluate the results\n",
    "- Train and validate your model on a large dataset\n",
    "\n",
    "Try running the notebook and see how well your model can make blurry images sharp!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
