{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92ec3f2-c118-457e-8b10-ccbbcf1eb7ef",
   "metadata": {},
   "source": [
    "# Introduction to Image Super Resolution\n",
    "\n",
    "This notebook demonstrates how to use deep learning to improve the resolution of images using the LAB color space. The workflow includes data preparation, model definition, training with progress visualization, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989e842",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae74b4d",
   "metadata": {},
   "source": [
    "Define the settings that will be used for the super-resolution experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8bdb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for training\n",
    "config = {\n",
    "    \"batch_size\": 16, # A batch is a set of images processed together by the model\n",
    "    \"adam_lr\": 1e-3, # The learning rate for the Adam optimizer sets how quickly the model learns (too quick can lead to instability)\n",
    "    \"loss_type\": \"mse\",  # or 'lpips' # The loss function measures how well the model's predictions match the ground truth (mse is mean squared error, lpips is perceptual loss)\n",
    "    \"resume_weights_path\": None,  # Path to checkpoint to resume from\n",
    "    \"num_epochs\": 10,\n",
    "    \"train_batches_per_epoch\": 64,\n",
    "    \"validation_batches_per_epoch\": 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088f2047",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "Import all necessary libraries for deep learning, image processing, visualization, and progress tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55834149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "from io import BytesIO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms\n",
    "from skimage import color\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "import lpips\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9d1506",
   "metadata": {},
   "source": [
    "## Why Use the LAB Color Space?\n",
    "The LAB color space separates the lightness (L) from the color information (A and B channels). This helps our model focus on making the image brighter and clearer, while keeping the colors accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb85fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lab_tensor(img: Image.Image) -> torch.Tensor:\n",
    "    arr = np.asarray(img.convert(\"RGB\"), dtype=np.float32) / 255.0\n",
    "    lab = color.rgb2lab(arr)\n",
    "    l_channel = lab[..., 0:1] / 100.0\n",
    "    a = lab[..., 1:2] / 128.0\n",
    "    b = lab[..., 2:3] / 128.0\n",
    "    lab_norm = np.concatenate([l_channel, a, b], axis=-1)\n",
    "    return torch.from_numpy(lab_norm.transpose(2, 0, 1).copy()).float()\n",
    "\n",
    "\n",
    "def to_numpy_img(lab_tensor: torch.Tensor) -> np.ndarray:\n",
    "    arr = lab_tensor.detach().cpu().numpy().transpose(1, 2, 0)\n",
    "    l_channel = arr[..., 0] * 100.0\n",
    "    a = arr[..., 1] * 128.0\n",
    "    b = arr[..., 2] * 128.0\n",
    "    lab = np.stack([l_channel, a, b], axis=-1)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", UserWarning)\n",
    "        rgb = color.lab2rgb(lab)\n",
    "    return np.clip(rgb, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bdf12b",
   "metadata": {},
   "source": [
    "## Preparing the Dataset\n",
    "Prepare the dataset by cropping, resizing, and converting images to LAB format. Define a streaming dataset class for efficient loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db1dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_stream(dataset, crop_size: int, scale: int):\n",
    "    lowres_size = crop_size // scale\n",
    "    crop = transforms.RandomCrop(crop_size)\n",
    "    for example in dataset:\n",
    "        img_data = example[\"image\"]\n",
    "        img = (\n",
    "            img_data\n",
    "            if isinstance(img_data, Image.Image)\n",
    "            else Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
    "        )\n",
    "        if min(img.size) < crop_size:\n",
    "            scale_factor = crop_size / min(img.size)\n",
    "            new_size = (\n",
    "                int(round(img.size[0] * scale_factor)),\n",
    "                int(round(img.size[1] * scale_factor)),\n",
    "            )\n",
    "            img = img.resize(new_size, resample=Image.Resampling.BICUBIC)\n",
    "        img_patch = crop(img)\n",
    "        lab_patch = to_lab_tensor(img_patch)\n",
    "        lab_patch_lowres = torch.nn.functional.interpolate(\n",
    "            lab_patch.unsqueeze(0),\n",
    "            size=(lowres_size, lowres_size),\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        ).squeeze(0)\n",
    "        yield lab_patch_lowres, lab_patch\n",
    "\n",
    "\n",
    "class SuperResStream(IterableDataset):\n",
    "    def __init__(self, dataset, crop_size: int, scale: int):\n",
    "        self.dataset = dataset\n",
    "        self.crop_size = crop_size\n",
    "        self.scale = scale\n",
    "\n",
    "    def __iter__(self):\n",
    "        return preprocess_stream(self.dataset, self.crop_size, self.scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e1a7f1",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "Load the training and validation datasets using Hugging Face Datasets in streaming mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493f4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(\n",
    "    \"imagenet-1k\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "val_dataset = load_dataset(\n",
    "    \"imagenet-1k\",\n",
    "    split=\"validation\",\n",
    "    streaming=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_size = 128\n",
    "scale = 2\n",
    "train_stream = SuperResStream(train_dataset, crop_size, scale)\n",
    "val_stream = SuperResStream(val_dataset, crop_size, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c722057a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_stream,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_stream,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09d928",
   "metadata": {},
   "source": [
    "## Build the Deep Super-Resolution Model\n",
    "Define the deep neural network for super-resolution, focusing on the L (lightness) channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939a3b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSuperResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=7, padding=3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        body_layers = []\n",
    "        for _ in range(8):\n",
    "            body_layers.append(nn.Conv2d(128, 128, kernel_size=3, padding=1))\n",
    "            body_layers.append(nn.ReLU(inplace=True))\n",
    "        self.body = nn.Sequential(*body_layers)\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 512, kernel_size=3, padding=1),\n",
    "            nn.PixelShuffle(2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.tail = nn.Conv2d(\n",
    "            128,\n",
    "            1,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.head(x)\n",
    "        x = self.body(x)\n",
    "        x = self.upsample(x)\n",
    "        x = self.tail(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b99706",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DeepSuperResNet().to(device)\n",
    "print(\"Model summary:\")\n",
    "print(\n",
    "    summary(\n",
    "        model,\n",
    "        input_size=(config[\"batch_size\"], 3, 64, 64),\n",
    "        col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    "        depth=4,\n",
    "        row_settings=(\"var_names\",),\n",
    "    )\n",
    ")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d98e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=config.get(\"adam_lr\", 1e-3))\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba85f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"loss_type\"] == \"lpips\":\n",
    "    lpips_loss_fn = lpips.LPIPS(net=\"vgg\", spatial=False).to(device)\n",
    "\n",
    "    def criterion(pred, target):\n",
    "        pred = 2 * pred - 1\n",
    "        target = 2 * target - 1\n",
    "        return lpips_loss_fn(pred, target).mean()\n",
    "elif config[\"loss_type\"] == \"mse\":\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "\n",
    "    def criterion(pred, target):\n",
    "        return mse_loss_fn(pred, target)\n",
    "else:\n",
    "    raise ValueError(f\"Unknown loss_type: {config['loss_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d605d9a",
   "metadata": {},
   "source": [
    "## Visualize Progress\n",
    "Let's visualize the results of the last batch from both training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273e9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_progress(model, lowres, highres, device, title=\"Output\", save_path=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lowres_AB = lowres[:, 1:3]\n",
    "        highres_L = highres[:, 0:1]\n",
    "        upsampled = torch.nn.functional.interpolate(\n",
    "            lowres,\n",
    "            size=highres.shape[-2:],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        pred_L = model(upsampled.to(device)).cpu()\n",
    "        up_AB = torch.nn.functional.interpolate(\n",
    "            lowres_AB,\n",
    "            size=highres_L.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        if pred_L.shape[-2:] != up_AB.shape[-2:]:\n",
    "            pred_L = torch.nn.functional.interpolate(\n",
    "                pred_L,\n",
    "                size=up_AB.shape[-2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        outputs = torch.cat([pred_L, up_AB], dim=1)\n",
    "        n = min(6, lowres.size(0))\n",
    "        fig, axes = plt.subplots(3, n, figsize=(2.5 * n, 8))\n",
    "        for i in range(n):\n",
    "            axes[0, i].imshow(to_numpy_img(lowres[i]))\n",
    "            axes[0, i].set_title(\"Low-res\", fontsize=10)\n",
    "            axes[1, i].imshow(to_numpy_img(outputs[i]))\n",
    "            axes[1, i].set_title(\"Output\", fontsize=10)\n",
    "            axes[2, i].imshow(to_numpy_img(highres[i]))\n",
    "            axes[2, i].set_title(\"High-res\", fontsize=10)\n",
    "            for row in range(3):\n",
    "                axes[row, i].axis(\"off\")\n",
    "        plt.suptitle(title)\n",
    "        plt.tight_layout()\n",
    "        if save_path is not None:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def plot_loss_curves(train_losses, val_losses, save_path):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(1, len(train_losses)+1), train_losses, label=\"Train Loss\", marker='o')\n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "    plt.semilogy()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training and Validation Loss Curves\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "output_dir = Path(\"results\")\n",
    "output_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c88e84",
   "metadata": {},
   "source": [
    "## Training Loop: One Epoch\n",
    "Let's run one epoch of training. We'll process a set number of batches, update the model, and keep track of the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c2e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(config[\"num_epochs\"]):\n",
    "    prefix = f\"Epoch {epoch+1}/{config['num_epochs']}\"\n",
    "    model.train()\n",
    "    running_loss, batch_count = 0.0, 0\n",
    "    train_iterator = iter(train_loader)\n",
    "    last_lowres, last_highres = None, None\n",
    "    train_bar = tqdm(range(config[\"train_batches_per_epoch\"]), desc=f\"{prefix} Training\", leave=False)\n",
    "    for _ in train_bar:\n",
    "        try:\n",
    "            lowres, highres = next(train_iterator)\n",
    "        except StopIteration:\n",
    "            train_iterator = iter(train_loader)\n",
    "            lowres, highres = next(train_iterator)\n",
    "        lowres = lowres.to(device)\n",
    "        highres = highres.to(device)\n",
    "        upsampled = torch.nn.functional.interpolate(\n",
    "            lowres,\n",
    "            size=highres.shape[-2:],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "        output_L = model(upsampled)\n",
    "        highres_L = highres[:, 0:1]\n",
    "        if output_L.shape[-2:] != highres_L.shape[-2:]:\n",
    "            output_L = torch.nn.functional.interpolate(\n",
    "                output_L,\n",
    "                size=highres_L.shape[-2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        loss = criterion(output_L, highres_L)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        last_lowres, last_highres = (\n",
    "            lowres.detach().cpu(),\n",
    "            highres.detach().cpu(),\n",
    "        )\n",
    "        train_bar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    avg_loss = running_loss / batch_count if batch_count > 0 else float(\"nan\")\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_running_loss, val_batch_count = 0.0, 0\n",
    "    val_iterator = iter(val_loader)\n",
    "    val_last_lowres, val_last_highres = None, None\n",
    "    val_bar = tqdm(range(config[\"validation_batches_per_epoch\"]), desc=f\"{prefix} Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for _ in val_bar:\n",
    "            try:\n",
    "                val_lowres, val_highres = next(val_iterator)\n",
    "            except StopIteration:\n",
    "                val_iterator = iter(val_loader)\n",
    "                val_lowres, val_highres = next(val_iterator)\n",
    "            val_lowres = val_lowres.to(device)\n",
    "            val_highres = val_highres.to(device)\n",
    "            val_upsampled = torch.nn.functional.interpolate(\n",
    "                val_lowres,\n",
    "                size=val_highres.shape[-2:],\n",
    "                mode=\"bicubic\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "            val_output_L = model(val_upsampled)\n",
    "            val_highres_L = val_highres[:, 0:1]\n",
    "            if val_output_L.shape[-2:] != val_highres_L.shape[-2:]:\n",
    "                val_output_L = torch.nn.functional.interpolate(\n",
    "                    val_output_L,\n",
    "                    size=val_highres_L.shape[-2:],\n",
    "                    mode=\"bicubic\",\n",
    "                    align_corners=False,\n",
    "                )\n",
    "            val_loss = criterion(val_output_L, val_highres_L)\n",
    "            val_running_loss += val_loss.item()\n",
    "            val_batch_count += 1\n",
    "            val_last_lowres, val_last_highres = (\n",
    "                val_lowres.detach().cpu(),\n",
    "                val_highres.detach().cpu(),\n",
    "            )\n",
    "            val_bar.set_postfix({\"loss\": f\"{val_loss.item():.4f}\"})\n",
    "    val_avg_loss = (\n",
    "        val_running_loss / val_batch_count if val_batch_count > 0 else float(\"nan\")\n",
    "    )\n",
    "    val_losses.append(val_avg_loss)\n",
    "    print(f\"{prefix} - avg train loss: {avg_loss:.4f} | avg val loss: {val_avg_loss:.4f}\")\n",
    "\n",
    "    # Visualize and save progress for this epoch\n",
    "    img_save_path = output_dir/f\"epoch_{epoch+1:03d}_progress.png\"\n",
    "    visualize_progress(model, last_lowres, last_highres, device, title=f\"{prefix} Training Batch Output\", save_path=img_save_path)\n",
    "\n",
    "    # Plot and save the loss curves after each epoch\n",
    "    loss_curve_save_path = output_dir/\"loss_curve.png\"\n",
    "    plot_loss_curves(train_losses, val_losses, loss_curve_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bbf0c3",
   "metadata": {},
   "source": [
    "## Save Model Weights\n",
    "Save the trained model weights for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba4534",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_dir = output_dir / \"weights\"\n",
    "weights_dir.mkdir(exist_ok=True)\n",
    "weights_path = weights_dir / \"superres_model_final.pth\"\n",
    "torch.save(model.state_dict(), str(weights_path))\n",
    "print(f\"Model weights saved to {weights_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadcf7cb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook covers image preparation, model building, training with progress visualization, and evaluation for deep learning-based image super-resolution in the LAB color space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b411da5",
   "metadata": {},
   "source": [
    "## Full Validation Image Inference\n",
    "Apply the trained model to an entire validation image (not just a patch). We'll select an image by index, process it through the model, and visualize the original high-res, low-res, and super-resolved results side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a24039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a validation image by index\n",
    "val_img_index = 2  # Change this index to try different images\n",
    "\n",
    "# Load the image from the validation set\n",
    "val_dataset_iter = iter(val_dataset)\n",
    "for i in range(val_img_index + 1):\n",
    "    example = next(val_dataset_iter)\n",
    "img_data = example[\"image\"]\n",
    "img = img_data if isinstance(img_data, Image.Image) else Image.open(BytesIO(img_data)).convert(\"RGB\")\n",
    "\n",
    "# Convert to LAB and generate low-res version\n",
    "lab = color.rgb2lab(np.asarray(img, dtype=np.float32) / 255.0)\n",
    "l_channel = lab[..., 0:1] / 100.0\n",
    "ab_channels = lab[..., 1:3] / 128.0\n",
    "lab_norm = np.concatenate([l_channel, ab_channels], axis=-1)\n",
    "highres_lab_tensor = torch.from_numpy(lab_norm.transpose(2, 0, 1)).unsqueeze(0).float()\n",
    "\n",
    "# Generate low-res version\n",
    "lowres_size = (img.height // scale, img.width // scale)\n",
    "lowres_lab = torch.nn.functional.interpolate(\n",
    "    highres_lab_tensor,\n",
    "    size=lowres_size,\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "# Upsample back to original size for model input\n",
    "upsampled_lab = torch.nn.functional.interpolate(\n",
    "    lowres_lab,\n",
    "    size=highres_lab_tensor.shape[-2:],\n",
    "    mode=\"bicubic\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "# Model expects batch, 3, H, W\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pred_L = model(upsampled_lab.to(device)).cpu()\n",
    "    # If needed, resize pred_L to match AB channels\n",
    "    if pred_L.shape[-2:] != upsampled_lab[:, 1:3].shape[-2:]:\n",
    "        pred_L = torch.nn.functional.interpolate(\n",
    "            pred_L,\n",
    "            size=upsampled_lab[:, 1:3].shape[-2:],\n",
    "            mode=\"bicubic\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "    # Combine predicted L with upsampled AB\n",
    "    up_AB = upsampled_lab[:, 1:3]\n",
    "    output_lab = torch.cat([pred_L, up_AB], dim=1)\n",
    "\n",
    "# Convert tensors back to numpy RGB images for visualization\n",
    "orig_rgb = np.clip(color.lab2rgb(lab), 0, 1)\n",
    "lowres_lab_np = lowres_lab.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "lowres_lab_np[..., 0] *= 100.0\n",
    "lowres_lab_np[..., 1:] *= 128.0\n",
    "lowres_rgb = np.clip(color.lab2rgb(lowres_lab_np), 0, 1)\n",
    "output_lab_np = output_lab.squeeze(0).cpu().numpy().transpose(1, 2, 0)\n",
    "output_lab_np[..., 0] *= 100.0\n",
    "output_lab_np[..., 1:] *= 128.0\n",
    "output_rgb = np.clip(color.lab2rgb(output_lab_np), 0, 1)\n",
    "\n",
    "# Plot and save the results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "axes[0].imshow(orig_rgb)\n",
    "axes[0].set_title(\"Original High-Res\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(lowres_rgb)\n",
    "axes[1].set_title(\"Low-Res Input\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(output_rgb)\n",
    "axes[2].set_title(\"Super-Resolved Output\")\n",
    "axes[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "fullimg_save_path = output_dir / f\"val_fullimg_{val_img_index:03d}_result.png\"\n",
    "plt.savefig(fullimg_save_path)\n",
    "plt.show()\n",
    "print(f\"Full image result saved to {fullimg_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
